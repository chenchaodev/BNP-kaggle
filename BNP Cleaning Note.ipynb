{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Workflow for Binary Probability\n",
    "- 增加1列表示缺失值，用 0 表示数据正常，1 表示确实数据\n",
    "- 原数据列中缺失值用-9999表示\n",
    "- 对 object 进行处理，当 object 值种类小于 150 时，利用 `pd.get_dummies()` 对此进行矢量化\n",
    "- 删除所有 object 数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "#scikit learn ensembe workflow for binary probability\n",
    "import time; start_time = time.time()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss, make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import random; random.seed(2016)\n",
    "\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "num_train = train.shape[0]\n",
    "\n",
    "y_train = train['target']\n",
    "train = train.drop(['target'],axis=1)\n",
    "id_test = test['ID']\n",
    "\n",
    "def fill_nan_null(val):\n",
    "    ret_fill_nan_null = 0.0\n",
    "    if val == True:\n",
    "        ret_fill_nan_null = 1.0\n",
    "    return ret_fill_nan_null\n",
    "\n",
    "df_all = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "df_all['null_count'] = df_all.isnull().sum(axis=1).tolist()\n",
    "df_all_temp = df_all['ID']\n",
    "df_all = df_all.drop(['ID'],axis=1)\n",
    "df_data_types = df_all.dtypes[:] #{'object':0,'int64':0,'float64':0,'datetime64':0}\n",
    "d_col_drops = []\n",
    "\n",
    "for i in range(len(df_data_types)):\n",
    "    df_all[str(df_data_types.index[i])+'_nan_'] = df_all[str(df_data_types.index[i])].map(lambda x:fill_nan_null(pd.isnull(x)))\n",
    "df_all = df_all.fillna(-9999)\n",
    "#df_all = df_all.replace(0, -9999)\n",
    "\n",
    "for i in range(len(df_data_types)):\n",
    "    if str(df_data_types[i])=='object':\n",
    "        df_u = pd.unique(df_all[str(df_data_types.index[i])].ravel())\n",
    "        print(\"Column: \", str(df_data_types.index[i]), \" Length: \", len(df_u))\n",
    "        d={}\n",
    "        j = 1000\n",
    "        for s in df_u:\n",
    "            d[str(s)]=j\n",
    "            j+=5\n",
    "        df_all[str(df_data_types.index[i])+'_vect_'] = df_all[str(df_data_types.index[i])].map(lambda x:d[str(x)])\n",
    "        d_col_drops.append(str(df_data_types.index[i]))\n",
    "        if len(df_u)<150:\n",
    "            dummies = pd.get_dummies(df_all[str(df_data_types.index[i])]).rename(columns=lambda x: str(df_data_types.index[i]) + '_' + str(x))\n",
    "            df_all_temp = pd.concat([df_all_temp, dummies], axis=1)\n",
    "\n",
    "df_all_temp = df_all_temp.drop(['ID'],axis=1)\n",
    "df_all = pd.concat([df_all, df_all_temp], axis=1)\n",
    "print(len(df_all), len(df_all.columns))\n",
    "#df_all.to_csv(\"df_all.csv\")\n",
    "train = df_all.iloc[:num_train]\n",
    "test = df_all.iloc[num_train:]\n",
    "train = train.drop(d_col_drops,axis=1)\n",
    "test = test.drop(d_col_drops,axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtraTreesClassifier002\n",
    "- 删除一些列（原因不明）\n",
    "- object 处理，`pd.factorize`,其中 nan 转化为 -1\n",
    "- 剩余 nan 处理，填充为 -999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import ensemble\n",
    "\n",
    "\n",
    "print('Load data...')\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "target = train['target'].values\n",
    "train = train.drop(['ID','target','v8','v23','v25','v36','v37','v46','v51','v53','v54','v63','v73','v75','v79','v81','v82','v89','v92','v95','v105','v107','v108','v109','v110','v116','v117','v118','v119','v123','v124','v128'],axis=1)\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "id_test = test['ID'].values\n",
    "test = test.drop(['ID','v8','v23','v25','v36','v37','v46','v51','v53','v54','v63','v73','v75','v79','v81','v82','v89','v92','v95','v105','v107','v108','v109','v110','v116','v117','v118','v119','v123','v124','v128'],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Clearing...')\n",
    "for (train_name, train_series), (test_name, test_series) in zip(train.iteritems(),test.iteritems()):\n",
    "    if train_series.dtype == 'O':\n",
    "        #for objects: factorize\n",
    "        train[train_name], tmp_indexer = pd.factorize(train[train_name])\n",
    "        test[test_name] = tmp_indexer.get_indexer(test[test_name])\n",
    "        #but now we have -1 values (NaN)\n",
    "    else:\n",
    "        #for int or float: fill NaN\n",
    "        tmp_len = len(train[train_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            #print \"mean\", train_series.mean()\n",
    "            train.loc[train_series.isnull(), train_name] = -999 \n",
    "        #and Test\n",
    "        tmp_len = len(test[test_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            test.loc[test_series.isnull(), test_name] = -999\n",
    "\n",
    "X_train = train\n",
    "X_test = test\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNP Correlation & Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "\n",
    "# Imports\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "# numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import cross_validation\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# float & string columns\n",
    "\n",
    "for f in bnp_df.columns:\n",
    "    # fill NaN values with mean\n",
    "    if bnp_df[f].dtype == 'float64':\n",
    "        bnp_df[f][np.isnan(bnp_df[f])] = bnp_df[f].mean()\n",
    "        test_df[f][np.isnan(test_df[f])] = test_df[f].mean()\n",
    "        \n",
    "    # fill NaN values with most occured value\n",
    "    elif bnp_df[f].dtype == 'object':\n",
    "        bnp_df[f][bnp_df[f] != bnp_df[f]] = bnp_df[f].value_counts().index[0]\n",
    "        test_df[f][test_df[f] != test_df[f]] = test_df[f].value_counts().index[0]\n",
    "        \n",
    "        \n",
    "# There are some columns with non-numerical values(i.e. dtype='object'),\n",
    "# So, We will create a corresponding unique numerical value for each non-numerical value in a column of training and testing set.\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "for f in bnp_df.columns:\n",
    "    if bnp_df[f].dtype == 'object':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(np.unique(list(bnp_df[f].values) + list(test_df[f].values)))\n",
    "        bnp_df[f]   = lbl.transform(list(bnp_df[f].values))\n",
    "        test_df[f]  = lbl.transform(list(test_df[f].values))\n",
    "        \n",
    "# define training and testing sets\n",
    "\n",
    "X_train = bnp_df.drop([\"ID\",\"target\"],axis=1)\n",
    "Y_train = bnp_df[\"target\"]\n",
    "X_test  = test_df.drop(\"ID\",axis=1).copy()\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "logreg.score(X_train, Y_train)\n",
    "\n",
    "# Xgboost \n",
    "\n",
    "params = {\"objective\": \"binary:logistic\"}\n",
    "\n",
    "T_train_xgb = xgb.DMatrix(X_train, Y_train)\n",
    "X_test_xgb  = xgb.DMatrix(X_test)\n",
    "\n",
    "gbm = xgb.train(params, T_train_xgb, 20)\n",
    "Y_pred = gbm.predict(X_test_xgb)\n",
    "\n",
    "# get Coefficient of Determination(R^2) for each feature using Logistic Regression\n",
    "coeff_df = DataFrame(bnp_df.columns.delete([0,1]))\n",
    "coeff_df.columns = ['Features']\n",
    "coeff_df[\"Coefficient Estimate\"] = (pd.Series(logreg.coef_[0])) ** 2\n",
    "\n",
    "# preview\n",
    "coeff_df.head()\n",
    "\n",
    "# Plot coefficient of determination in order\n",
    "\n",
    "coeff_ser = Series(list(coeff_df[\"Coefficient Estimate\"]), index=coeff_df[\"Features\"]).sort_values()\n",
    "fig = coeff_ser.plot(kind='barh', figsize=(15,5))\n",
    "fig.set(ylim=(116, 131))\n",
    "\n",
    "# Create submission\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission[\"ID\"]            = test_df[\"ID\"]\n",
    "submission[\"PredictedProb\"] = Y_pred\n",
    "\n",
    "submission.to_csv('bnp.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Documents\\\\DMProject\\\\BNP'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "if (os.path.isdir('D:\\\\Documents\\\\DMProject\\\\BNP\\\\')):\n",
    "    os.chdir('D:\\\\Documents\\\\DMProject\\\\BNP\\\\')\n",
    "if (os.path.isdir('C:\\\\Users\\\\Chao\\\\Documents\\\\DMProject\\\\BNP\\\\')):\n",
    "    os.chdir('C:\\\\Users\\\\Chao\\\\Documents\\\\DMProject\\\\BNP\\\\')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114321, 114393)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('input\\\\train.csv')\n",
    "test = pd.read_csv('input\\\\test.csv')\n",
    "target = train['target']\n",
    "id_test = test['ID']\n",
    "\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(['ID', 'target'], axis=1)\n",
    "test = test.drop(['ID'], axis=1)\n",
    "column_names = train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((114321, 131), (114393, 131))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat((train, test), axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228714, 131)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data_types = (df_all.dtypes[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10199212"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.isnull().sum(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_nan_null(val):\n",
    "    ret_fill_nan_null = 0.0\n",
    "    if val == True:\n",
    "        ret_fill_nan_null = 1.0\n",
    "    return ret_fill_nan_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(df_data_types)):\n",
    "    df_all[str(df_data_types.index[i])+'_nan_'] = df_all[str(df_data_types.index[i])].map(lambda x:fill_nan_null(pd.isnull(x)))\n",
    "df_all = df_all.fillna(-9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column:  v3  Length:  4\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_all_temp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-18456aba6260>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_u\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mdummies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_data_types\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_data_types\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mdf_all_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_all_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummies\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_all_temp' is not defined"
     ]
    }
   ],
   "source": [
    "d_col_drops = []\n",
    "for i in range(len(df_data_types)):\n",
    "    if str(df_data_types[i])=='object':\n",
    "        df_u = pd.unique(df_all[str(df_data_types.index[i])].ravel())\n",
    "        print(\"Column: \", str(df_data_types.index[i]), \" Length: \", len(df_u))\n",
    "        d={}\n",
    "        j = 1000\n",
    "        for s in df_u:\n",
    "            d[str(s)]=j\n",
    "            j+=5\n",
    "        df_all[str(df_data_types.index[i])+'_vect_'] = df_all[str(df_data_types.index[i])].map(lambda x:d[str(x)])\n",
    "        d_col_drops.append(str(df_data_types.index[i]))\n",
    "        if len(df_u)<150:\n",
    "            dummies = pd.get_dummies(df_all[str(df_data_types.index[i])]).rename(columns=lambda x: str(df_data_types.index[i]) + '_' + str(x))\n",
    "            df_all_temp = pd.concat([df_all_temp, dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5097471"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scikit learn ensembe workflow for binary probability\n",
    "import time; start_time = time.time()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss, make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import random; random.seed(2016)\n",
    "\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "num_train = train.shape[0]\n",
    "\n",
    "y_train = train['target']\n",
    "train = train.drop(['target'],axis=1)\n",
    "id_test = test['ID']\n",
    "\n",
    "def fill_nan_null(val):\n",
    "    ret_fill_nan_null = 0.0\n",
    "    if val == True:\n",
    "        ret_fill_nan_null = 1.0\n",
    "    return ret_fill_nan_null\n",
    "\n",
    "df_all = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "df_all['null_count'] = df_all.isnull().sum(axis=1).tolist()\n",
    "df_all_temp = df_all['ID']\n",
    "df_all = df_all.drop(['ID'],axis=1)\n",
    "df_data_types = df_all.dtypes[:] #{'object':0,'int64':0,'float64':0,'datetime64':0}\n",
    "d_col_drops = []\n",
    "\n",
    "for i in range(len(df_data_types)):\n",
    "    df_all[str(df_data_types.index[i])+'_nan_'] = df_all[str(df_data_types.index[i])].map(lambda x:fill_nan_null(pd.isnull(x)))\n",
    "df_all = df_all.fillna(-9999)\n",
    "#df_all = df_all.replace(0, -9999)\n",
    "\n",
    "for i in range(len(df_data_types)):\n",
    "    if str(df_data_types[i])=='object':\n",
    "        df_u = pd.unique(df_all[str(df_data_types.index[i])].ravel())\n",
    "        print(\"Column: \", str(df_data_types.index[i]), \" Length: \", len(df_u))\n",
    "        d={}\n",
    "        j = 1000\n",
    "        for s in df_u:\n",
    "            d[str(s)]=j\n",
    "            j+=5\n",
    "        df_all[str(df_data_types.index[i])+'_vect_'] = df_all[str(df_data_types.index[i])].map(lambda x:d[str(x)])\n",
    "        d_col_drops.append(str(df_data_types.index[i]))\n",
    "        if len(df_u)<150:\n",
    "            dummies = pd.get_dummies(df_all[str(df_data_types.index[i])]).rename(columns=lambda x: str(df_data_types.index[i]) + '_' + str(x))\n",
    "            df_all_temp = pd.concat([df_all_temp, dummies], axis=1)\n",
    "\n",
    "df_all_temp = df_all_temp.drop(['ID'],axis=1)\n",
    "df_all = pd.concat([df_all, df_all_temp], axis=1)\n",
    "print(len(df_all), len(df_all.columns))\n",
    "#df_all.to_csv(\"df_all.csv\")\n",
    "train = df_all.iloc[:num_train]\n",
    "test = df_all.iloc[num_train:]\n",
    "train = train.drop(d_col_drops,axis=1)\n",
    "test = test.drop(d_col_drops,axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
