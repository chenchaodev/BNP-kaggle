{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Chao\\\\Documents\\\\DMProject\\\\BNP'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "if (os.path.isdir('D:\\\\Documents\\\\DMProject\\\\BNP\\\\')):\n",
    "    os.chdir('D:\\\\Documents\\\\DMProject\\\\BNP\\\\')\n",
    "if (os.path.isdir('C:\\\\Users\\\\Chao\\\\Documents\\\\DMProject\\\\BNP\\\\')):\n",
    "    os.chdir('C:\\\\Users\\\\Chao\\\\Documents\\\\DMProject\\\\BNP\\\\')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('input\\\\train.csv')\n",
    "test = pd.read_csv('input\\\\test.csv')\n",
    "target = train['target']\n",
    "id_test = test['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(['ID', 'target'], axis=1)\n",
    "test = test.drop(['ID'], axis=1)\n",
    "column_names = train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = train.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((228714, 131), (114321, 131), (114393, 131))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape, train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_data_types = all_data.dtypes[:] #{'object':0,'int64':0,'float64':0,'datetime64':0}\n",
    "d_col_drops = []\n",
    "d_col_group_too_big = []\n",
    "num_col_names = []\n",
    "str_col_names = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1 processed!\n",
      "v2 processed!\n",
      "v3 processed!\n",
      "v4 processed!\n",
      "v5 processed!\n",
      "v6 processed!\n",
      "v7 processed!\n",
      "v8 processed!\n",
      "v9 processed!\n",
      "v10 processed!\n",
      "v11 processed!\n",
      "v12 processed!\n",
      "v13 processed!\n",
      "v14 processed!\n",
      "v15 processed!\n",
      "v16 processed!\n",
      "v17 processed!\n",
      "v18 processed!\n",
      "v19 processed!\n",
      "v20 processed!\n",
      "v21 processed!\n",
      "v22 unique value to big 23420\n",
      "v22 processed!\n",
      "v23 processed!\n",
      "v24 processed!\n",
      "v25 processed!\n",
      "v26 processed!\n",
      "v27 processed!\n",
      "v28 processed!\n",
      "v29 processed!\n",
      "v30 processed!\n",
      "v31 processed!\n",
      "v32 processed!\n",
      "v33 processed!\n",
      "v34 processed!\n",
      "v35 processed!\n",
      "v36 processed!\n",
      "v37 processed!\n",
      "v38 processed!\n",
      "v39 processed!\n",
      "v40 processed!\n",
      "v41 processed!\n",
      "v42 processed!\n",
      "v43 processed!\n",
      "v44 processed!\n",
      "v45 processed!\n",
      "v46 processed!\n",
      "v47 processed!\n",
      "v48 processed!\n",
      "v49 processed!\n",
      "v50 processed!\n",
      "v51 processed!\n",
      "v52 processed!\n",
      "v53 processed!\n",
      "v54 processed!\n",
      "v55 processed!\n",
      "v56 processed!\n",
      "v57 processed!\n",
      "v58 processed!\n",
      "v59 processed!\n",
      "v60 processed!\n",
      "v61 processed!\n",
      "v62 processed!\n",
      "v63 processed!\n",
      "v64 processed!\n",
      "v65 processed!\n",
      "v66 processed!\n",
      "v67 processed!\n",
      "v68 processed!\n",
      "v69 processed!\n",
      "v70 processed!\n",
      "v71 processed!\n",
      "v72 processed!\n",
      "v73 processed!\n",
      "v74 processed!\n",
      "v75 processed!\n",
      "v76 processed!\n",
      "v77 processed!\n",
      "v78 processed!\n",
      "v79 processed!\n",
      "v80 processed!\n",
      "v81 processed!\n",
      "v82 processed!\n",
      "v83 processed!\n",
      "v84 processed!\n",
      "v85 processed!\n",
      "v86 processed!\n",
      "v87 processed!\n",
      "v88 processed!\n",
      "v89 processed!\n",
      "v90 processed!\n",
      "v91 processed!\n",
      "v92 processed!\n",
      "v93 processed!\n",
      "v94 processed!\n",
      "v95 processed!\n",
      "v96 processed!\n",
      "v97 processed!\n",
      "v98 processed!\n",
      "v99 processed!\n",
      "v100 processed!\n",
      "v101 processed!\n",
      "v102 processed!\n",
      "v103 processed!\n",
      "v104 processed!\n",
      "v105 processed!\n",
      "v106 processed!\n",
      "v107 processed!\n",
      "v108 processed!\n",
      "v109 processed!\n",
      "v110 processed!\n",
      "v111 processed!\n",
      "v112 processed!\n",
      "v113 processed!\n",
      "v114 processed!\n",
      "v115 processed!\n",
      "v116 processed!\n",
      "v117 processed!\n",
      "v118 processed!\n",
      "v119 processed!\n",
      "v120 processed!\n",
      "v121 processed!\n",
      "v122 processed!\n",
      "v123 processed!\n",
      "v124 processed!\n",
      "v125 processed!\n",
      "v126 processed!\n",
      "v127 processed!\n",
      "v128 processed!\n",
      "v129 processed!\n",
      "v130 processed!\n",
      "v131 processed!\n"
     ]
    }
   ],
   "source": [
    "for col_name in column_names:\n",
    "    if (all_data[col_name].dtype != 'object'):\n",
    "        num_col_names.append(col_name)\n",
    "        new_col_name_nan = col_name + '_' + 'nan'\n",
    "        all_data[new_col_name_nan] = 0\n",
    "        all_data.loc[pd.isnull(all_data[col_name]).values, new_col_name_nan] = 1\n",
    "        all_data[col_name].fillna(all_data[col_name].mean(), inplace=True)                \n",
    "    else:\n",
    "        col_value_u = pd.unique(all_data[col_name].ravel())\n",
    "        if len(col_value_u) > 200:\n",
    "            num_col_names.append(col_name)\n",
    "            # factor replace\n",
    "            print(\"%s unique value to big %d\" % (col_name, len(col_value_u)))\n",
    "            \n",
    "            d_col_group_too_big.append(col_name)\n",
    "            \n",
    "            new_col_name_nan = col_name + '_' + 'nan'\n",
    "            all_data[new_col_name_nan] = 0\n",
    "            all_data.loc[pd.isnull(all_data[col_name]).values, new_col_name_nan] = 1\n",
    "            \n",
    "            all_data[col_name], tmp_indexer = pd.factorize(all_data[col_name], na_sentinel=-1)\n",
    "            mean_col = all_data[col_name][all_data[col_name] != -1].mean()\n",
    "            all_data.loc[all_data[col_name] == -1, col_name] = mean_col\n",
    "                         \n",
    "        else:\n",
    "            str_col_names.append(col_name)\n",
    "            # dummies 因子化\n",
    "            col_dummies = pd.get_dummies(all_data[col_name],  prefix=col_name, prefix_sep='_', dummy_na=True)\n",
    "            for col_value in col_dummies.columns:\n",
    "                all_data[col_value] = col_dummies[col_value]\n",
    "                \n",
    "            #             all_data = pd.concat([all_data, col_dummies], axis=1)\n",
    "#             g = all_data.groupby(col_name) \n",
    "#             for name, group in g:\n",
    "#                 new_col_name = col_name + \"_\" + name\n",
    "#                 all_data[new_col_name] = 0\n",
    "#                 all_data.loc[all_data[col_name] == name, new_col_name] = 1\n",
    "                \n",
    "#             new_col_name_nan = col_name + '_' + 'nan'\n",
    "#             all_data[new_col_name_nan] = 0\n",
    "#             all_data.loc[pd.isnull(all_data[col_name]).values, new_col_name_nan] = 1\n",
    "            all_data.drop(col_name, axis=1, inplace=True)\n",
    "    print(\"%s processed!\" % (col_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228714, 620)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all_data.fillna(all_data.mean(), inplace=True)\n",
    "# all_data.shape\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "features_to_scale = all_data[num_col_names]\n",
    "features_to_scale = scaler.fit_transform(features_to_scale.values)\n",
    "scaled_future = pd.DataFrame(features_to_scale,columns= num_col_names, index=all_data.index)\n",
    "for col_name in num_col_names:\n",
    "    all_data[col_name] = scaled_future[col_name]\n",
    "\n",
    "\n",
    "# for col_name in num_col_names:\n",
    "#     if (all_data[col_name].dtype != 'object'): \n",
    "#         print(col_name)\n",
    "#         scaler = preprocessing.StandardScaler()\n",
    "#         scale_param = scaler.fit(all_data[col_name].values)\n",
    "#         all_data[col_name] = scaler.fit_transform(all_data[col_name], scale_param)\n",
    "        \n",
    "        \n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# features = df[cols]\n",
    "# features = StandardScaler().fit_transform(features.values)\n",
    "# scaled_features = pd.DataFrame(features,columns= cols, index=df.index)\n",
    "# for col in cols:\n",
    "#     df[col]= scaled_features[col]    \n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228714, 620)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((114321, 620), (114393, 620))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_len = len(train)\n",
    "train_all_pred = all_data[:train_len]\n",
    "test_all_pred = all_data[train_len:]\n",
    "train_all_pred.shape, test_all_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_all_pred.to_csv('pred/train_all_pred_26.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_all_pred.to_csv('pred/test_all_pred_26.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = open('pred/target_all_pred_26.pkl', 'wb')\n",
    "\n",
    "# Pickle dictionary using protocol 0.\n",
    "pickle.dump(target, output)\n",
    "\n",
    "output.close()\n",
    "\n",
    "output = open('pred/id_test_all_pred_26.pkl', 'wb')\n",
    "\n",
    "# Pickle dictionary using protocol 0.\n",
    "pickle.dump(id_test, output)\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
